{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Downloading all needed models and importing libs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install stanza"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: stanza in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (1.2.3)\n",
      "Requirement already satisfied: requests in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from stanza) (2.24.0)\n",
      "Requirement already satisfied: protobuf in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from stanza) (3.17.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from stanza) (1.8.1)\n",
      "Requirement already satisfied: numpy in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from stanza) (1.19.2)\n",
      "Requirement already satisfied: tqdm in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from stanza) (4.50.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from requests->stanza) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from requests->stanza) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from requests->stanza) (3.0.4)\n",
      "Requirement already satisfied: six>=1.9 in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from protobuf->stanza) (1.15.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/olegmelnikov/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma,depparse,ner')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/olegmelnikov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: 140kB [00:00, 7.58MB/s]                    \n",
      "2021-09-06 14:42:52 INFO: Downloading default packages for language: ru (Russian)...\n",
      "2021-09-06 14:42:53 INFO: File exists: /Users/olegmelnikov/stanza_resources/ru/default.zip.\n",
      "2021-09-06 14:42:57 INFO: Finished downloading models and saved to /Users/olegmelnikov/stanza_resources.\n",
      "2021-09-06 14:42:57 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2021-09-06 14:42:57 INFO: Use device: cpu\n",
      "2021-09-06 14:42:57 INFO: Loading: tokenize\n",
      "2021-09-06 14:42:57 INFO: Loading: pos\n",
      "2021-09-06 14:42:57 INFO: Loading: lemma\n",
      "2021-09-06 14:42:57 INFO: Loading: depparse\n",
      "2021-09-06 14:42:57 INFO: Loading: ner\n",
      "2021-09-06 14:42:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stanza parse example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "doc = nlp('Необходимы знания таких технологий как Python, Java, PostgreSQL')\n",
    "print(doc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"Необходимы\",\n",
      "      \"lemma\": \"необходимый\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"feats\": \"Degree=Pos|Number=Plur|Variant=Short\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 10,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"знания\",\n",
      "      \"lemma\": \"знание\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"feats\": \"Animacy=Inan|Case=Nom|Gender=Neut|Number=Plur\",\n",
      "      \"head\": 1,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 11,\n",
      "      \"end_char\": 17,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"таких\",\n",
      "      \"lemma\": \"такой\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"feats\": \"Case=Gen|Number=Plur\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 18,\n",
      "      \"end_char\": 23,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"технологий\",\n",
      "      \"lemma\": \"технология\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"feats\": \"Animacy=Inan|Case=Gen|Gender=Fem|Number=Plur\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"nmod\",\n",
      "      \"start_char\": 24,\n",
      "      \"end_char\": 34,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"как\",\n",
      "      \"lemma\": \"как\",\n",
      "      \"upos\": \"SCONJ\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"mark\",\n",
      "      \"start_char\": 35,\n",
      "      \"end_char\": 38,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"Python\",\n",
      "      \"lemma\": \"Python\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"feats\": \"Foreign=Yes\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"flat:foreign\",\n",
      "      \"start_char\": 39,\n",
      "      \"end_char\": 45,\n",
      "      \"ner\": \"S-MISC\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 45,\n",
      "      \"end_char\": 46,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"Java\",\n",
      "      \"lemma\": \"Java\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"feats\": \"Foreign=Yes\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"flat:foreign\",\n",
      "      \"start_char\": 47,\n",
      "      \"end_char\": 51,\n",
      "      \"ner\": \"S-MISC\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"head\": 10,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 51,\n",
      "      \"end_char\": 52,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \"PostgreSQL\",\n",
      "      \"lemma\": \"PostgreSQL\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"feats\": \"Foreign=Yes\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"flat:foreign\",\n",
      "      \"start_char\": 53,\n",
      "      \"end_char\": 63,\n",
      "      \"ner\": \"S-MISC\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Raw vacancy parsing & processing methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def del_stopwords(doc):\n",
    "    r = doc.copy()\n",
    "    cur = 0\n",
    "    for ind, i in enumerate(doc):\n",
    "        if i['lemma'] in russian_stopwords or i['lemma'] in punctuation:\n",
    "            r.pop(ind - cur)\n",
    "            cur += 1\n",
    "    return r\n",
    "\n",
    "def doc_to_list(doc):\n",
    "    doc_json = json.loads(str(doc))\n",
    "    return doc_json[0]\n",
    "\n",
    "def lemmatized_sentence(doc):\n",
    "    arr = []\n",
    "    for i in doc:\n",
    "        arr.append(i['lemma'])\n",
    "    return ' '.join(arr)\n",
    "\n",
    "def process_doc(doc):\n",
    "    doc = doc_to_list(doc)\n",
    "    doc = del_stopwords(doc)\n",
    "    return doc\n",
    "\n",
    "def find_all_tech(doc):\n",
    "    ans = []\n",
    "    for i in doc:\n",
    "        if i['ner'][-4:] == 'MISC' and i['upos'] == 'PROPN':\n",
    "            ans.append(i['lemma'])\n",
    "    return list(set(ans))\n",
    "\n",
    "def get_lemms(doc):\n",
    "    ans = []\n",
    "    for i in doc:\n",
    "        ans.append(i['lemma'].lower())\n",
    "    return ans\n",
    "\n",
    "def check_if_flexible(lemms):\n",
    "    return ('гибкий' in lemms or 'удобный' in lemms or 'свободный' in lemms) and ('день' in lemms or 'график' in lemms or 'время' in lemms)\n",
    "\n",
    "def get_specializations(lemms):\n",
    "    keywords = [['docker', 'kubernetes', 'devops', 'развертывание', 'jenkins', 'ansible', 'ci', 'cd'], ['ml', 'dl', 'ds', 'machine', 'learning', 'data' ,'science', 'tensorflow', 'keras', 'pytorch', 'kaggle', 'comuter', 'vision'], ['test', 'testing', 'тест', 'тестирование', 'тестировщик']]\n",
    "    ans = [0, 0, 0, 0]\n",
    "    for i in lemms:\n",
    "        for ind, block in enumerate(keywords):\n",
    "            if i in block:\n",
    "                ans[ind] += 1\n",
    "    sm = ans[0] + ans[1] + ans[2]\n",
    "    if sm == 0:\n",
    "        ans[3] = 1\n",
    "    return ans\n",
    "\n",
    "def get_full_result(doc):\n",
    "    doc = process_doc(doc)\n",
    "    lemms = get_lemms(doc)\n",
    "    techs = find_all_tech(doc)\n",
    "    flexible = check_if_flexible(lemms)\n",
    "    specs = get_specializations(lemms)\n",
    "    return techs, flexible, specs\n",
    "\n",
    "def clear_string(s):\n",
    "    s = s.strip().replace('\\n', '').replace('\\r', '')\n",
    "    s = re.sub('[\"«»;?!,()]', '', s)\n",
    "    s = re.sub('[/—-]', ' ', s)\n",
    "    s = re.sub(r\"\\\\\", ' ', s)\n",
    "    s = re.sub('  ', ' ', s)\n",
    "    rus_alphavite = 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\n",
    "    for c in rus_alphavite:\n",
    "        s = s.replace(c, c.lower())\n",
    "    return s\n",
    "\n",
    "def check_eng(html_doc):\n",
    "    a = re.search('[а-яА-Я]', html_doc)\n",
    "    if a is None:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_lemms2(doc):\n",
    "    ans = []\n",
    "    for i in doc:\n",
    "        ans.append(i['lemma'])\n",
    "    return ans\n",
    "\n",
    "def select_spec(a):\n",
    "    if a[1] > 1:\n",
    "        return 'Data Science'\n",
    "    if a[2] > 1:\n",
    "        return 'Тестирование'\n",
    "    if a[0] > 2:\n",
    "        return 'DevOps'\n",
    "    return 'Разработка'\n",
    "\n",
    "\n",
    "# main html processing func, parse vacancy on simple sections\n",
    "def parse_html(html_doc):\n",
    "    if check_eng(html_doc):\n",
    "        print(\"This vacancy is English\")\n",
    "        return []\n",
    "    MAX_TITLE_LEN = 5\n",
    "    MAX_BODY_LEN = 10\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    # process beginning part before <strong>\n",
    "    first_ul = soup.ul\n",
    "    try:\n",
    "        first_strong = first_ul.find_previous_sibling('p')\n",
    "    except Exception as e:\n",
    "        first_strong = soup.find_all('p')[-1]\n",
    "    if first_strong is None:\n",
    "        first_strong = first_ul.find_previous_sibling('strong')\n",
    "    informal_part = []\n",
    "    current_p = first_strong\n",
    "    result = []\n",
    "    try:\n",
    "        while current_p is not None and current_p.find_previous_sibling() is not None:\n",
    "            next_p = current_p.find_previous_sibling()\n",
    "\n",
    "            text = clear_string(next_p.text)\n",
    "            if len(text) == 0:\n",
    "                break\n",
    "            words = get_lemms2(process_doc(nlp(text)))\n",
    "            length = min(MAX_BODY_LEN, len(words))\n",
    "            body = ' '.join(words[: length])\n",
    "            informal_part.append(body)\n",
    "            current_p = next_p\n",
    "        informal_part.reverse()\n",
    "        for paragraph in informal_part:\n",
    "            result.append(['', paragraph])\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # process <strong> and next <ul>\n",
    "    ptr = 1 if len(result) > 0 else 0\n",
    "    for ul in soup.find_all('ul'):\n",
    "        try:\n",
    "            text = clear_string(ul.find_previous_sibling().text)\n",
    "            words = get_lemms2(process_doc(nlp(text)))\n",
    "            length = min(MAX_TITLE_LEN, len(words))\n",
    "            title = words[:length]\n",
    "            title = ' '.join(title)\n",
    "            items = [] \n",
    "            for li in ul.find_all('li'):\n",
    "                s = clear_string(li.text)\n",
    "                \n",
    "                words = get_lemms2(process_doc(nlp(s)))\n",
    "                length = min(MAX_BODY_LEN, len(words))\n",
    "                body = words[:length]\n",
    "                body = ' '.join(body)\n",
    "                items.append(body)\n",
    "\n",
    "            for i in items:\n",
    "                result.append([title, i])\n",
    "            ptr += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df_train = pd.read_csv('train_dataset_201.csv', index_col=0)\n",
    "df_train_numpy = df_train.to_numpy()\n",
    "x = df_train_numpy[:, 0]\n",
    "y = df_train_numpy[:, 1].astype('int')\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', KNeighborsClassifier(n_neighbors = 6,weights = 'distance',algorithm = 'brute'))])\n",
    "model_knn = pipe.fit(x, y)\n",
    "\n",
    "def classify_section(title):\n",
    "    prediction = model_knn.predict([title])[0]\n",
    "    return prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Facts extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def extract_facts_from_vacancy_sections(sections, key_skills):\n",
    "    ans = [[], [], False, [0, 0, 0, 0]]\n",
    "    for i in sections:\n",
    "        target = classify_section(i[0])\n",
    "        doc = nlp(i[1])\n",
    "        res = get_full_result(doc)\n",
    "        if target == 1:\n",
    "            ans[0] += res[0]\n",
    "        if target == 2:\n",
    "            ans[1] += res[0]\n",
    "        if res[1]:\n",
    "            ans[2] = True\n",
    "        for j in range(4):\n",
    "            ans[3][j] += res[2][j]\n",
    "    \n",
    "    ans[0] = list(set(ans[0] + key_skills))\n",
    "    ans[1] = list(set(ans[1]))\n",
    "    ans[3] = select_spec(ans[3])\n",
    "    d = {'Обязательные компетенции': ans[0], 'Желательные компетенции': ans[1], 'Гибкий график работы': ans[2], 'Подобласть': ans[3]}\n",
    "    return d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# main function to call\n",
    "def extract_facts_from_vacancy(vacancy_id):\n",
    "    url = f'https://api.hh.ru/vacancies/{vacancy_id}'\n",
    "    vacancy = requests.get(url).json()\n",
    "    html_doc = vacancy['description']\n",
    "    res1 = parse_html(html_doc)\n",
    "    key_skills = []\n",
    "    for i in vacancy['key_skills']:\n",
    "        key_skills.append(i['name'])\n",
    "    return extract_facts_from_vacancy_sections(res1, key_skills)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Demonstration of model's results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# input here id of any vacancy on hh.ru\n",
    "vacancy_id = 40612238\n",
    "\n",
    "# then get it's facts extracted\n",
    "print(extract_facts_from_vacancy(vacancy_id))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Обязательные компетенции': ['Mysql', 'PostgreSQL', 'Flask', 'Python', 'FastApi', 'Tornado'], 'Желательные компетенции': ['Go'], 'Гибкий график работы': False, 'Подобласть': 'Разработка'}\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "ac04af24cf61e25f3c174b1468d18facbc6a5bd015644e51cd1be2934a39a15d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}